{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part2_Using_hugging_face_Library.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynCMLZQqgWHq"
      },
      "source": [
        "## Training SOTA tokenizer models using HuggingFace `tokenizers` package\n",
        "\n",
        "\n",
        "1. Byte Pair Encoding (BPE) Algorithm\n",
        "2. WordPiece Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96Z883c-jjhT",
        "outputId": "963d6da8-eece-40aa-fb8b-4ea7044552c5"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.11.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77_XrM_Vgq8M"
      },
      "source": [
        "## Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlEbro2djoFY"
      },
      "source": [
        "## importing the tokenizer and subword BPE trainer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
        "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
        "                                WordPieceTrainer, UnigramTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "untCq1TSl20J"
      },
      "source": [
        "#### Download the data to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRAZgeTkolcn",
        "outputId": "a2968cf4-f8e5-43e9-f400-bb65f91eecb1"
      },
      "source": [
        "!wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-15 16:01:16--  http://www.gutenberg.org/cache/epub/16457/pg16457.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/cache/epub/16457/pg16457.txt [following]\n",
            "--2022-03-15 16:01:17--  https://www.gutenberg.org/cache/epub/16457/pg16457.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 617622 (603K) [text/plain]\n",
            "Saving to: ‚Äòpg16457.txt.2‚Äô\n",
            "\n",
            "pg16457.txt.2       100%[===================>] 603.15K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-03-15 16:01:17 (6.90 MB/s) - ‚Äòpg16457.txt.2‚Äô saved [617622/617622]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNPhkPE2nIIG",
        "outputId": "1f022dd8-6141-481b-f39b-567d9947ce0c"
      },
      "source": [
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-15 16:01:20--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.107.126\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.107.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‚Äòwikitext-103-raw-v1.zip.2‚Äô\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  37.6MB/s    in 4.8s    \n",
            "\n",
            "2022-03-15 16:01:25 (37.8 MB/s) - ‚Äòwikitext-103-raw-v1.zip.2‚Äô saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "replace wikitext-103-raw/wiki.test.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wikitext-103-raw/wiki.test.raw  \n",
            "replace wikitext-103-raw/wiki.valid.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
            "replace wikitext-103-raw/wiki.train.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wikitext-103-raw/wiki.train.raw  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6GxCTfj0OWw"
      },
      "source": [
        "## Define the 3-step process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFmfGYSfrT5y"
      },
      "source": [
        "unk_token = \"<UNK>\"  # token for unknown words\n",
        "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]  # special tokens\n",
        "\n",
        "def prepare_tokenizer_trainer(alg):\n",
        "    \"\"\"\n",
        "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
        "    \"\"\"\n",
        "    if alg == 'BPE':\n",
        "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
        "        trainer = BpeTrainer(special_tokens = spl_tokens)\n",
        "    elif alg == 'UNI':\n",
        "        tokenizer = Tokenizer(Unigram())\n",
        "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)\n",
        "    elif alg == 'WPC':\n",
        "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
        "        trainer = WordPieceTrainer(special_tokens = spl_tokens)\n",
        "    else:\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
        "        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n",
        "    \n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    return tokenizer, trainer\n",
        "\n",
        "\n",
        "def train_tokenizer(files, alg='WLV'):\n",
        "    \"\"\"\n",
        "    Takes the files and trains the tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
        "    tokenizer.train(files, trainer) # training the tokenzier\n",
        "    tokenizer.save(\"./tokenizer-trained.json\")\n",
        "    tokenizer = Tokenizer.from_file(\"./tokenizer-trained.json\")\n",
        "    return tokenizer\n",
        "\n",
        "def tokenize(input_string, tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenizes the input string using the tokenizer provided.\n",
        "    \"\"\"\n",
        "    output = tokenizer.encode(input_string)\n",
        "    return output\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP1xuStV0fDl"
      },
      "source": [
        "## Training each model on gutenberg and wikitext dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWb05mfxv1jw",
        "outputId": "f156a423-15f6-4343-b961-139a3e79b391"
      },
      "source": [
        "gutenberg_dataset = ['pg16457.txt']\n",
        "wikitext_dataset = [f\"./wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "\n",
        "tokens_dict = {}\n",
        "\n",
        "for files in [gutenberg_dataset, wikitext_dataset]:\n",
        "    print(f\"========Using vocabulary from corpus {files}=======\")\n",
        "    for alg in ['BPE', 'WPC']:\n",
        "        trained_tokenizer = train_tokenizer(files, alg)\n",
        "        input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!üòç\"\n",
        "        output = tokenize(input_string, trained_tokenizer)\n",
        "        tokens_dict[alg] = output.tokens\n",
        "        print(\"Using \", alg, \" Algorithm\")\n",
        "        print(output.tokens, \"-> length of tokens :\", len(output.tokens))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========Using vocabulary from corpus ['pg16457.txt']=======\n",
            "Using  BPE  Algorithm\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', 'ken', 'ization', 't', 'ut', 'or', 'ial', '.', 'T', 'ok', 'en', 'ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'N', 'L', 'P', 'pi', 'pe', 'line', '.', 'We', 'will', 'be', 'comparing', 'the', 'to', 'k', 'ens', 'generated', 'by', 'each', 'to', 'ken', 'ization', 'model', '.', 'Ex', 'c', 'ited', 'much', '?', '!', '<UNK>'] -> length of tokens : 55\n",
            "Using  WPC  Algorithm\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', '##ken', '##ization', 't', '##ut', '##oria', '##l', '.', 'To', '##ken', '##ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'N', '##L', '##P', 'pip', '##el', '##ine', '.', 'We', 'will', 'be', 'comparing', 'the', 'to', '##ken', '##s', 'generated', 'by', 'each', 'to', '##ken', '##ization', 'model', '.', 'Ex', '##ci', '##ted', 'much', '<UNK>'] -> length of tokens : 52\n",
            "========Using vocabulary from corpus ['./wikitext-103-raw/wiki.test.raw', './wikitext-103-raw/wiki.train.raw', './wikitext-103-raw/wiki.valid.raw']=======\n",
            "Using  BPE  Algorithm\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', 'ken', 'ization', 'tut', 'orial', '.', 'Tok', 'en', 'ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'NL', 'P', 'pipeline', '.', 'We', 'will', 'be', 'comparing', 'the', 'tok', 'ens', 'generated', 'by', 'each', 'to', 'ken', 'ization', 'model', '.', 'Ex', 'cited', 'much', '?', '!', '<UNK>'] -> length of tokens : 47\n",
            "Using  WPC  Algorithm\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', '##ken', '##ization', 'tut', '##orial', '.', 'Tok', '##eni', '##za', '##ti', '##on', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'NL', '##P', 'pipeline', '.', 'We', 'will', 'be', 'comparing', 'the', 'to', '##ken', '##s', 'generated', 'by', 'each', 'to', '##ken', '##ization', 'model', '.', 'Exc', '##ited', 'much', '<UNK>'] -> length of tokens : 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNMPi3802ac2"
      },
      "source": [
        "## Comparing the BPE and Unigram tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUYLXYt626O2"
      },
      "source": [
        "\n",
        "tokens_dict = {}\n",
        "\n",
        "for alg in ['BPE', 'UNI', 'WPC']:\n",
        "    trained_tokenizer = train_tokenizer(wikitext_dataset, alg)\n",
        "    input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!üòç\"\n",
        "    output = tokenize(input_string, trained_tokenizer)\n",
        "    tokens_dict[alg] = output.tokens"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZm0Za-v7l0l"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "max_len = max(len(tokens_dict['UNI']), len(tokens_dict['WPC']), len(tokens_dict['BPE']))\n",
        "diff_bpe = max_len - len(tokens_dict['BPE'])\n",
        "diff_wpc = max_len - len(tokens_dict['WPC'])\n",
        "\n",
        "tokens_dict['BPE'] = tokens_dict['BPE'] + ['<PAD>']*diff_bpe\n",
        "tokens_dict['WPC'] = tokens_dict['WPC'] + ['<PAD>']*diff_wpc\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(tokens_dict)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "Pq4HsfdhBU-N",
        "outputId": "040ffa74-4653-4f8b-ac0a-9c1e3e47e584"
      },
      "source": [
        "df.head(47)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          BPE      UNI        WPC\n",
              "0        This     This       This\n",
              "1          is        i         is\n",
              "2           a        s          a\n",
              "3        deep        a       deep\n",
              "4    learning     deep   learning\n",
              "5          to    learn         to\n",
              "6         ken      ing      ##ken\n",
              "7     ization        t  ##ization\n",
              "8         tut        o        tut\n",
              "9       orial      ken    ##orial\n",
              "10          .  ization          .\n",
              "11        Tok        t        Tok\n",
              "12         en        u      ##eni\n",
              "13    ization        t       ##za\n",
              "14         is        o       ##ti\n",
              "15        the     rial       ##on\n",
              "16      first        .         is\n",
              "17       step        T        the\n",
              "18         in        o      first\n",
              "19          a      ken       step\n",
              "20       deep  ization         in\n",
              "21   learning        i          a\n",
              "22         NL        s       deep\n",
              "23          P      the   learning\n",
              "24   pipeline    first         NL\n",
              "25          .     step        ##P\n",
              "26         We       in   pipeline\n",
              "27       will        a          .\n",
              "28         be     deep         We\n",
              "29  comparing    learn       will\n",
              "30        the      ing         be\n",
              "31        tok        N  comparing\n",
              "32        ens        L        the\n",
              "33  generated        P         to\n",
              "34         by        p      ##ken\n",
              "35       each        i        ##s\n",
              "36         to        p  generated\n",
              "37        ken        e         by\n",
              "38    ization     line       each\n",
              "39      model        .         to\n",
              "40          .        W      ##ken\n",
              "41         Ex        e  ##ization\n",
              "42      cited     will      model\n",
              "43       much       be          .\n",
              "44          ?      com        Exc\n",
              "45          !      par     ##ited\n",
              "46      <UNK>      ing       much"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8867dc04-aadb-4f24-ae39-c064c429e387\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BPE</th>\n",
              "      <th>UNI</th>\n",
              "      <th>WPC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This</td>\n",
              "      <td>This</td>\n",
              "      <td>This</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>s</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>deep</td>\n",
              "      <td>a</td>\n",
              "      <td>deep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>learning</td>\n",
              "      <td>deep</td>\n",
              "      <td>learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>to</td>\n",
              "      <td>learn</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ken</td>\n",
              "      <td>ing</td>\n",
              "      <td>##ken</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ization</td>\n",
              "      <td>t</td>\n",
              "      <td>##ization</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tut</td>\n",
              "      <td>o</td>\n",
              "      <td>tut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>orial</td>\n",
              "      <td>ken</td>\n",
              "      <td>##orial</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>.</td>\n",
              "      <td>ization</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Tok</td>\n",
              "      <td>t</td>\n",
              "      <td>Tok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>en</td>\n",
              "      <td>u</td>\n",
              "      <td>##eni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ization</td>\n",
              "      <td>t</td>\n",
              "      <td>##za</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>is</td>\n",
              "      <td>o</td>\n",
              "      <td>##ti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>the</td>\n",
              "      <td>rial</td>\n",
              "      <td>##on</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>first</td>\n",
              "      <td>.</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>step</td>\n",
              "      <td>T</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>in</td>\n",
              "      <td>o</td>\n",
              "      <td>first</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>a</td>\n",
              "      <td>ken</td>\n",
              "      <td>step</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>deep</td>\n",
              "      <td>ization</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>learning</td>\n",
              "      <td>i</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>NL</td>\n",
              "      <td>s</td>\n",
              "      <td>deep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>P</td>\n",
              "      <td>the</td>\n",
              "      <td>learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>pipeline</td>\n",
              "      <td>first</td>\n",
              "      <td>NL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>.</td>\n",
              "      <td>step</td>\n",
              "      <td>##P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>We</td>\n",
              "      <td>in</td>\n",
              "      <td>pipeline</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>will</td>\n",
              "      <td>a</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>be</td>\n",
              "      <td>deep</td>\n",
              "      <td>We</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>comparing</td>\n",
              "      <td>learn</td>\n",
              "      <td>will</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>the</td>\n",
              "      <td>ing</td>\n",
              "      <td>be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>tok</td>\n",
              "      <td>N</td>\n",
              "      <td>comparing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>ens</td>\n",
              "      <td>L</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>generated</td>\n",
              "      <td>P</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>by</td>\n",
              "      <td>p</td>\n",
              "      <td>##ken</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>each</td>\n",
              "      <td>i</td>\n",
              "      <td>##s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>to</td>\n",
              "      <td>p</td>\n",
              "      <td>generated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>ken</td>\n",
              "      <td>e</td>\n",
              "      <td>by</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>ization</td>\n",
              "      <td>line</td>\n",
              "      <td>each</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>model</td>\n",
              "      <td>.</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>.</td>\n",
              "      <td>W</td>\n",
              "      <td>##ken</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Ex</td>\n",
              "      <td>e</td>\n",
              "      <td>##ization</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>cited</td>\n",
              "      <td>will</td>\n",
              "      <td>model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>much</td>\n",
              "      <td>be</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>?</td>\n",
              "      <td>com</td>\n",
              "      <td>Exc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>!</td>\n",
              "      <td>par</td>\n",
              "      <td>##ited</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>&lt;UNK&gt;</td>\n",
              "      <td>ing</td>\n",
              "      <td>much</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8867dc04-aadb-4f24-ae39-c064c429e387')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8867dc04-aadb-4f24-ae39-c064c429e387 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8867dc04-aadb-4f24-ae39-c064c429e387');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "lGMUMZeS4YQD",
        "outputId": "16a5b4a2-b151-44fb-db99-aa5a6a72c38a"
      },
      "source": [
        "df.describe(include= 'all')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          BPE UNI    WPC\n",
              "count      68  68     68\n",
              "unique     37  41     37\n",
              "top     <PAD>   o  <PAD>\n",
              "freq       21   5     20"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c62eaf4-2e22-46d3-abf0-22ef5c45627c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BPE</th>\n",
              "      <th>UNI</th>\n",
              "      <th>WPC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>68</td>\n",
              "      <td>68</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>37</td>\n",
              "      <td>41</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "      <td>o</td>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c62eaf4-2e22-46d3-abf0-22ef5c45627c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c62eaf4-2e22-46d3-abf0-22ef5c45627c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c62eaf4-2e22-46d3-abf0-22ef5c45627c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTCeB6Ka8p2n",
        "outputId": "225a313f-53e5-4e43-b44a-680fd4e73b79"
      },
      "source": [
        "set(df['UNI']) - set(df['BPE'])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'L',\n",
              " 'N',\n",
              " 'T',\n",
              " 'W',\n",
              " 'com',\n",
              " 'd',\n",
              " 'e',\n",
              " 'generate',\n",
              " 'i',\n",
              " 'ing',\n",
              " 'learn',\n",
              " 'line',\n",
              " 'o',\n",
              " 'p',\n",
              " 'par',\n",
              " 'rial',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'üòç'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJrxr9uW_8Gf",
        "outputId": "f58d67bc-2196-451a-d731-5f902a8b9530"
      },
      "source": [
        "set(df['UNI']) - set(df['WPC'])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!',\n",
              " '?',\n",
              " 'Ex',\n",
              " 'L',\n",
              " 'N',\n",
              " 'P',\n",
              " 'T',\n",
              " 'W',\n",
              " 'cited',\n",
              " 'com',\n",
              " 'd',\n",
              " 'e',\n",
              " 'generate',\n",
              " 'i',\n",
              " 'ing',\n",
              " 'ization',\n",
              " 'ken',\n",
              " 'learn',\n",
              " 'line',\n",
              " 'o',\n",
              " 'p',\n",
              " 'par',\n",
              " 'rial',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'üòç'}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhKiRtkFHW_P",
        "outputId": "94b97ec0-90ba-4deb-8586-5f13866bb1c5"
      },
      "source": [
        "set(df['WPC']) - set(df['UNI'])\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'##P',\n",
              " '##eni',\n",
              " '##ited',\n",
              " '##ization',\n",
              " '##ken',\n",
              " '##on',\n",
              " '##orial',\n",
              " '##s',\n",
              " '##ti',\n",
              " '##za',\n",
              " '<PAD>',\n",
              " '<UNK>',\n",
              " 'Exc',\n",
              " 'NL',\n",
              " 'Tok',\n",
              " 'We',\n",
              " 'comparing',\n",
              " 'generated',\n",
              " 'is',\n",
              " 'learning',\n",
              " 'pipeline',\n",
              " 'to',\n",
              " 'tut'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYbFvw4Z12fj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}